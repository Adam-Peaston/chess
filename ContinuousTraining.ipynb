{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a5e8dc",
   "metadata": {},
   "source": [
    "## Plan for continuous training\n",
    "\n",
    "Here's the plan. The script boots up and checks a continuous play directory which is populated with a number of training round sub-directores. Each sub-directory is labelled in sequence and contains a self_play sub-sub-directory. The script takes stock of the contents of the latest training round self_play directory.\n",
    "\n",
    "If the number of files (tournaments) saved in the self_play directory of the latest training round is less than some minimum M, say m < M then the self-play script is kicked off to play M - m more tournaments, completing the M requred self-play tournaments and saving to the  subdirectory.\n",
    "\n",
    "If the number of files saved in the self_play directory of the latest training round m >= M then the script checks if there is already a model saved in the training round directory.\n",
    "\n",
    "If there is not a model saved in the training round directory, the script will kick off a training routine to create a new model (or load a saved model from a possibly existing previous round), compile a training dataset from the most recent k training rounds (augmented with the catalogue of known checkmate positions) and train the model to stopping. The script will then save the model in the training round sub-directory.\n",
    "\n",
    "Then the script will create a new training round directory, labelled with the next integer in the training round sequence, containing a self_play directory. At this point we can essentially continue the loop from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae9f755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Current: round_9, already played: 80, left to play: -30\n",
      "Training on 238383 examples in 239 batches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [00:51<00:00,  4.63it/s]\n",
      "100%|██████████| 60/60 [00:08<00:00,  6.90it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'round_9\\\\model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Projects\\chess\\ContinuousTraining.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/chess/ContinuousTraining.ipynb#W1sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39m# Train on the data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/chess/ContinuousTraining.ipynb#W1sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     model_dest \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(current_training_round_dir, \u001b[39m'\u001b[39m\u001b[39mmodel.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Projects/chess/ContinuousTraining.ipynb#W1sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     model \u001b[39m=\u001b[39m train(model, loss_fn, optimizer, train_loader, test_loader, warmup_passes\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, max_lr\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m, save_dir\u001b[39m=\u001b[39;49mmodel_dest, stopping\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/chess/ContinuousTraining.ipynb#W1sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# Create next training round directory containing self_play sub-directory, and start loop from the top.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/chess/ContinuousTraining.ipynb#W1sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m next_index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplit(current_training_round_dir)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32me:\\Projects\\chess\\chess_model.py:354\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loss_fn, optimizer, train_dataloader, test_dataloader, warmup_passes, max_lr, save_dir, stopping)\u001b[0m\n\u001b[0;32m    351\u001b[0m mean_test_loss \u001b[39m=\u001b[39m loss_sum \u001b[39m/\u001b[39m examples_seen\n\u001b[0;32m    353\u001b[0m \u001b[39mif\u001b[39;00m mean_test_loss \u001b[39m<\u001b[39m best_test_loss:\n\u001b[1;32m--> 354\u001b[0m     torch\u001b[39m.\u001b[39;49msave(model\u001b[39m.\u001b[39;49mstate_dict(), save_dir)\n\u001b[0;32m    355\u001b[0m     best_test_loss \u001b[39m=\u001b[39m mean_test_loss\n\u001b[0;32m    356\u001b[0m     stopping_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\peast\\anaconda3\\lib\\site-packages\\torch\\serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[39m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \n\u001b[0;32m    342\u001b[0m \u001b[39mSaves an object to a disk file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[39m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m--> 376\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    377\u001b[0m     \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    378\u001b[0m         \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[1;32mc:\\Users\\peast\\anaconda3\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\peast\\anaconda3\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'round_9\\\\model.pt'"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "from torch.utils.data import DataLoader\n",
    "from chess_selfplay import harvest_checkmates\n",
    "from chess_model import TransformerModel, ChessDataset, TanhLoss, train\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "min_tournaments_each_round = 50 # A new model roughly once a day.\n",
    "root_dir = os.path.join('data','output')\n",
    "dir_sort_key = lambda d: int(d.split('_')[-1])\n",
    "\n",
    "## LOOP STARTS - just kill the machine any old time when you have stuff to do and you can fire it up again whenever you're ready.\n",
    "while True:\n",
    "\n",
    "    training_round_dirs = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir,d))], key=dir_sort_key)\n",
    "    current_training_round_dir = training_round_dirs[-1]\n",
    "    previous_training_round_dir = training_round_dirs[-2]\n",
    "\n",
    "    current_self_play_path = os.path.join(root_dir, current_training_round_dir, 'self_play')\n",
    "    if os.path.exists(current_self_play_path) and os.path.isdir(current_self_play_path):\n",
    "        number_games_played_already = len(os.listdir(current_self_play_path))\n",
    "    else:\n",
    "        os.mkdir(current_self_play_path)\n",
    "        number_games_played_already = 0\n",
    "\n",
    "    tournaments_left_to_play = min_tournaments_each_round - number_games_played_already\n",
    "    print(f'Current: {current_training_round_dir}, already played: {number_games_played_already}, left to play: {tournaments_left_to_play}')\n",
    "\n",
    "    if tournaments_left_to_play > 0:\n",
    "        \n",
    "        # self-play script to play tournaments_left_to_play more tournaments, saving to current_self_play_dir.\n",
    "        model_source_path = os.path.join(root_dir, previous_training_round_dir, 'model.pt')\n",
    "        assert os.path.exists(model_source_path) and os.path.isfile(model_source_path), 'ERROR: MODEL NOT FOUND.'\n",
    "\n",
    "        # Same base model with different look-ahead strength configuration \n",
    "        model_kwargs = {'nlayers':6, 'nheads':3, 'embed_dim':18, 'dk':5, 'device':device, 'load_path':model_source_path}\n",
    "        agent0_spec = {'type': 'transformer', 'kwargs': model_kwargs, 'num_simgames': 150, 'max_simmoves': 4, 'C': 1, 'p': 0.4, 'k': float('inf')}\n",
    "        agent1_spec = {'type': 'transformer', 'kwargs': model_kwargs, 'num_simgames':  1, 'max_simmoves': 1, 'C': 1, 'p': 0.4, 'k': float('inf')}\n",
    "        self_play_args = {\n",
    "            'num_workers':2, 'num_tournaments': tournaments_left_to_play, 'agents_spec': [agent0_spec, agent1_spec], \n",
    "            'num_games':1, 'starting_state':None, 'max_moves':200, 'save':True, 'result_dest':current_self_play_path\n",
    "        }\n",
    "\n",
    "        # Let's play\n",
    "        print(f'Playing {tournaments_left_to_play} tournaments...')\n",
    "        %run -i \"chess_selfplay.py\"\n",
    "        print(f'Self-play complete.')\n",
    "\n",
    "    # Extract the checkmates from the current_self_play_dir tournament games and save them in the current_training_round_dir.\n",
    "    current_round_checkmates_path = os.path.join(root_dir, current_training_round_dir, 'checkmates.pkl')\n",
    "    if not (os.path.exists(current_round_checkmates_path) and os.path.isfile(current_round_checkmates_path)):\n",
    "        _ = harvest_checkmates(os.path.join(root_dir, current_training_round_dir))\n",
    "\n",
    "    latest_model_path = os.path.join(root_dir, current_training_round_dir, 'model.pt')\n",
    "    if not (os.path.exists(latest_model_path) and os.path.isfile(latest_model_path)):\n",
    "\n",
    "        # No model saved here yet. Create and train a new model based on the previous k rounds of self-play data.\n",
    "        # We currently have 591 tournaments saved in baseline and 191 in round1. We could use k = 10 and go from there?\n",
    "        model_kwargs = {'nlayers':6, 'nheads':3, 'embed_dim':18, 'dk':5, 'device':device, 'load_path':None} # Brand new model\n",
    "        model = TransformerModel(**model_kwargs)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0, weight_decay=0)\n",
    "        loss_fn = TanhLoss()\n",
    "        dataset = ChessDataset(root_dir=root_dir, look_back=10, device=device)\n",
    "        train_set, test_set = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)])\n",
    "        train_loader = DataLoader(train_set, batch_size=1000, shuffle=True, num_workers=0)\n",
    "        test_loader = DataLoader(test_set, batch_size=1000, shuffle=True, num_workers=0)\n",
    "        print(f'Training on {len(train_set)} examples in {len(train_loader)} batches.')\n",
    "\n",
    "        # Train on the data\n",
    "        model_dest = os.path.join(root_dir, current_training_round_dir, 'model.pt')\n",
    "        model = train(model, loss_fn, optimizer, train_loader, test_loader, warmup_passes=4, max_lr=1e-4, save_dir=model_dest, stopping=5)\n",
    "        \n",
    "    # Create next training round directory containing self_play sub-directory, and start loop from the top.\n",
    "    next_index = int(os.path.split(current_training_round_dir)[-1].split('_')[-1]) + 1\n",
    "    next_training_round_dir = f'round_{next_index}'\n",
    "    print(f'Creating next training round directory {next_training_round_dir}')\n",
    "    next_training_round_path = os.path.join(root_dir, next_training_round_dir)\n",
    "    os.mkdir(next_training_round_path)\n",
    "    next_training_round_self_play_path = os.path.join(next_training_round_path, 'self_play')\n",
    "    os.mkdir(next_training_round_self_play_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8bf91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
